{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I hope you have all finished week 3 assignements with zeal as it is important to know about algorithms before we actually start applying to our dataset. This is the final week of our assignment and after this, we shall be doing final evaluation of your submissions and then providing you respective certificates.\n",
    "\n",
    "We will start with applying the models, I will aplly logistic regression and explain things accordingly, your assignment for this week is to apply the rest two classifier models to the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(r\"C:\\Users\\insapab\\Desktop\\Python\\Projects\\Breast Cancer Project\\Breast-Cancer-Detection-using-Machine-Learning-master\\cancer dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "labelencoder_Y=LabelEncoder()\n",
    "df.iloc[:,1]=labelencoder_Y.fit_transform(df.iloc[:,1].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df.iloc[:,2:31].values \n",
    "Y=df.iloc[:,1].values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test= train_test_split(X,Y, test_size=0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc=StandardScaler()\n",
    "X_train=sc.fit_transform(X_train)\n",
    "X_test=sc.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We will start off by creating a function for applying logistic regression to our data. We will use the in-built modules present in the scikit learn library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logreg (X_train, Y_train):\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    log=LogisticRegression (random_state=0)\n",
    "    log.fit(X_train, Y_train)\n",
    "    print(\"Logistic Regression Training Accuracy:\", log.score(X_train, Y_train))\n",
    "    return log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will be seeing random state in the code. What is random state used for?\n",
    "\n",
    "If there is no randomstate provided the system will use a randomstate that is generated internally. So, when you run the program multiple times you might see different train/test data points and the behavior will be unpredictable. In case, you have an issue with your model you will not be able to recreate it as you do not know the random number that was generated when you ran the program.\n",
    "\n",
    "### If these codes are a bit overwhelming at the moment, do not worry, you will be doing Decision Tree classifier and Random Forest classifier on your own. Then it will be clearer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Training Accuracy: 0.9906103286384976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "logrex=logreg(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above given accuracy is when we apply the algorithm to the data which we have used for training, it is much obvious that it will be very close to 100% because we are training with that data. We will be finding out the accuracy of the testing data with the help of confusion matrix. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will apply this algorithm to our testing data as we had earlier applied to the training set and create a confusion matrix. \n",
    "\n",
    "### Confusion Matrix \n",
    "\n",
    "A confusion matrix is a summary of prediction results on a classification problem. The number of correct and incorrect predictions are summarized with count values and broken down by each class. This is the key to the confusion matrix. The confusion matrix shows the ways in which your classification model is confused when it makes predictions. It gives us insight not only into the errors being made by a classifier but more importantly the types of errors that are being made.\n",
    "\n",
    "#### Definition of the Terms:\n",
    "\n",
    "Positive (P) : Observation is positive (for example: is an apple).\n",
    "Negative (N) : Observation is not positive (for example: is not an apple).\n",
    "True Positive (TP) : Observation is positive, and is predicted to be positive.\n",
    "False Negative (FN) : Observation is positive, but is predicted negative.\n",
    "True Negative (TN) : Observation is negative, and is predicted to be negative.\n",
    "False Positive (FP) : Observation is negative, but is predicted positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rates that are often computed from a confusion matrix for a binary classifier:\n",
    "\n",
    "Accuracy: Overall, how often is the classifier correct?\n",
    "(TP+TN)/total\n",
    "\n",
    "Misclassification Rate: Overall, how often is it wrong?\n",
    "(FP+FN)/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[86  4]\n",
      " [ 3 50]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(Y_test, logrex.predict(X_test))\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this model\n",
    "1. true positive=86\n",
    "2. True negative=50\n",
    "3. False positive=4\n",
    "4. False negative=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy of logistic regression model= 0.951048951048951\n"
     ]
    }
   ],
   "source": [
    "TP=cm[0][0]\n",
    "TN=cm[1][1]\n",
    "FN=cm[1][0]\n",
    "FP=cm[0][1]\n",
    "print(\"Testing accuracy of logistic regression model=\", (TP+TN)/(TP+TN+FN+FP))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this way, I have applied the logistic regression model to our data. Now it is your turn to apply Decision tree classifier and Random forest classifier to this data and find out the accuracy of the model using confusion matrix as shown above and also comment on which model had higher accuracy and why. You can find the codes for applying these models online, please use them!\n",
    "With this, we come towards the end of the project and in week 5, we will be doing your final evaluation, taking feedbacks from you all and providing you certificates. \n",
    "All the best and hope we will do more projects in future! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Training Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# decision Tree classifier\n",
    "def DecTree (X_train, Y_train):\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    dct = DecisionTreeClassifier(random_state=0, criterion = 'entropy')\n",
    "    dct.fit(X_train, Y_train)\n",
    "    print(\"Decision Tree Training Accuracy:\", dct.score(X_train, Y_train))\n",
    "    return dct\n",
    "dct = DecTree(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy of decision Tree model= 0.9370629370629371\n"
     ]
    }
   ],
   "source": [
    "# confusion matrix for decision Tree\n",
    "cm = confusion_matrix(Y_test, dct.predict(X_test))\n",
    "TP=cm[0][0]\n",
    "TN=cm[1][1]\n",
    "FN=cm[1][0]\n",
    "FP=cm[0][1]\n",
    "print(\"Testing accuracy of decision Tree model=\", (TP+TN)/(TP+TN+FN+FP))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random forest Training Accuracy: 0.9953051643192489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# random forest classifier\n",
    "def RdmForest(X_train, Y_train):\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    rdf = RandomForestClassifier(random_state=0,criterion = 'entropy')\n",
    "    rdf.fit(X_train, Y_train)\n",
    "    print(\"Random forest Training Accuracy:\", rdf.score(X_train, Y_train))\n",
    "    return rdf\n",
    "rdf = RdmForest(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy of random forest model= 0.965034965034965\n"
     ]
    }
   ],
   "source": [
    "# confusion matrix for random forest\n",
    "cm = confusion_matrix(Y_test, rdf.predict(X_test))\n",
    "TP=cm[0][0]\n",
    "TN=cm[1][1]\n",
    "FN=cm[1][0]\n",
    "FP=cm[0][1]\n",
    "print(\"Testing accuracy of random forest model=\", (TP+TN)/(TP+TN+FN+FP))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Among all the classifer methods, Random forest has better accuracy for both training and testing data sets \n",
      " All the models have good accuracy in training stage it clearly indicates that they are free from bias error but decision tree and logistic regression models have poor testing accuracy than random forest, it indicates that the logistic regression and decision tree models have variance error \n",
      " *  For logistic regression training and test accuracy looks better but if input data has non-linear features then Bias and variance error may increases \n",
      " *  For decision tree, decision is taken based on single tree, though training accuracy is better, it tends to overfit and making testing accuracy poorer \n",
      " * Where, the random forest classifier is free from both bias and variance error compared to other methods\n"
     ]
    }
   ],
   "source": [
    "#which model had higher accuracy and why?\n",
    "print('* Among all the classifer methods, Random forest has better accuracy for both training and testing data sets \\n All the models have good accuracy in training stage it clearly indicates that they are free from bias error but decision tree and logistic regression models have poor testing accuracy than random forest, it indicates that the logistic regression and decision tree models have variance error \\n *  For logistic regression training and test accuracy looks better but if input data has non-linear features then Bias and variance error may increases \\n *  For decision tree, decision is taken based on single tree, though training accuracy is better, it tends to overfit and making testing accuracy poorer \\n * Where, the random forest classifier is free from both bias and variance error compared to other methods')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
