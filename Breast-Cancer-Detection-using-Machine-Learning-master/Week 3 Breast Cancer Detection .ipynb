{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(r\"C:\\Users\\insapab\\Desktop\\Python\\Projects\\Breast Cancer Project\\Breast-Cancer-Detection-using-Machine-Learning-master\\cancer dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "labelencoder_Y=LabelEncoder()\n",
    "df.iloc[:,1]=labelencoder_Y.fit_transform(df.iloc[:,1].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>842302</td>\n",
       "      <td>1</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>842517</td>\n",
       "      <td>1</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>84300903</td>\n",
       "      <td>1</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>84348301</td>\n",
       "      <td>1</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>84358402</td>\n",
       "      <td>1</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0    842302          1        17.99         10.38          122.80     1001.0   \n",
       "1    842517          1        20.57         17.77          132.90     1326.0   \n",
       "2  84300903          1        19.69         21.25          130.00     1203.0   \n",
       "3  84348301          1        11.42         20.38           77.58      386.1   \n",
       "4  84358402          1        20.29         14.34          135.10     1297.0   \n",
       "\n",
       "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0          0.11840           0.27760          0.3001              0.14710   \n",
       "1          0.08474           0.07864          0.0869              0.07017   \n",
       "2          0.10960           0.15990          0.1974              0.12790   \n",
       "3          0.14250           0.28390          0.2414              0.10520   \n",
       "4          0.10030           0.13280          0.1980              0.10430   \n",
       "\n",
       "   ...  radius_worst  texture_worst  perimeter_worst  area_worst  \\\n",
       "0  ...         25.38          17.33           184.60      2019.0   \n",
       "1  ...         24.99          23.41           158.80      1956.0   \n",
       "2  ...         23.57          25.53           152.50      1709.0   \n",
       "3  ...         14.91          26.50            98.87       567.7   \n",
       "4  ...         22.54          16.67           152.20      1575.0   \n",
       "\n",
       "   smoothness_worst  compactness_worst  concavity_worst  concave points_worst  \\\n",
       "0            0.1622             0.6656           0.7119                0.2654   \n",
       "1            0.1238             0.1866           0.2416                0.1860   \n",
       "2            0.1444             0.4245           0.4504                0.2430   \n",
       "3            0.2098             0.8663           0.6869                0.2575   \n",
       "4            0.1374             0.2050           0.4000                0.1625   \n",
       "\n",
       "   symmetry_worst  fractal_dimension_worst  \n",
       "0          0.4601                  0.11890  \n",
       "1          0.2750                  0.08902  \n",
       "2          0.3613                  0.08758  \n",
       "3          0.6638                  0.17300  \n",
       "4          0.2364                  0.07678  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now split the original data into dependent and indepndent dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df.iloc[:,2:31].values #features that help us determine if patient has cancer or not\n",
    "Y=df.iloc[:,1].values #this is the dataset containing our target variable which indicates diagnosis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data we use is usually split into training data and test data. The training set contains a known output and the model learns on this data in order to be generalized to other data later on. We have the test dataset (or subset) in order to test our model’s prediction on this subset.\n",
    "\n",
    "From Sklearn, sub-library model_selection, we will import the train_test_split so we can split to training and test sets.  The test_size inside the function indicates the percentage of the data that should be held over for testing. It’s usually around 80/20 or 70/30. The ratio is kept as such so that model does not overfit or underfit. \n",
    "Let us understand first what overfitting and underfitting means:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overfitting\n",
    "Overfitting means that model we trained has trained “too well” and is now, well, fit too closely to the training dataset. This usually happens when the model is too complex i.e. too many features/variables compared to the number of observations. This model will be very accurate on the training data but will probably be very not accurate on untrained or new data. It is because this model is not generalized, meaning you can generalize the results and can’t make any inferences on other data, which is, ultimately, what you are trying to do. Basically, when this happens, the model learns or describes the “noise” in the training data instead of the actual relationships between variables in the data. This noise, obviously, isn’t part in of any new dataset, and cannot be applied to it.\n",
    "\n",
    "#### Underfitting\n",
    "In contrast to overfitting, when a model is underfitted, it means that the model does not fit the training data and therefore misses the trends in the data. It also means the model cannot be generalized to new data. This is usually the result of a very simple model which does not have enough predictors/independent variables. It could also happen when, for example, we fit a linear model ,like linear regression to data that is not linear. It almost goes without saying that this model will have poor predictive ability on training data and can’t be generalized to other data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test= train_test_split(X,Y, test_size=0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the code that you can use to test and split data using scikit learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1: Now, there is a small homeowrk for statistics wherein you have to read about the parameteres, define them in brief and write about two main types of distributions: \n",
    "#### 1. Gaussian distribution \n",
    "#### 2. Binomial distribution \n",
    "#### Differentiate between both as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Gaussian distribution : \n",
      " 1. it is a probability distribution that is symmetric about the mean, showing that data near the mean(u) are more frequent in occurrence than data far from the mean. In graph form, normal distribution will appear as a bell curve\n",
      " 2. Assuming u is mean and s is the standard deviation of a particular data then its probability density function is (1/(s*sqrt(2*pi))*exp(-0.5*((x-u)/s)^2) \n",
      " 3. Unlike binomial distribution, it can be applied to event which gives any possible outcome (even fractional value) so it is continuos probabilty distribution \n",
      " 4. It will fit for majority of natural events \n",
      "Binomial distribution : \n",
      " 1. It is applied only to those events which has only two mutual exculsive(cant occure same time)outcomes which have same probaility of occurance \n",
      " ex: tossing a coin which outcomes either HEADS or TAILS so it is discrete distribution function \n",
      " 2. Assuming p is probality of sucess, then its probability density function for observing x sucesses in N trails is NCx*p^x*(1-p)^(N-x) , where C represents the combination \n"
     ]
    }
   ],
   "source": [
    "# Gaussian distribution\n",
    "print(' Gaussian distribution : \\n 1. it is a probability distribution that is symmetric about the mean, showing that data near the mean(u) are more frequent in occurrence than data far from the mean. In graph form, normal distribution will appear as a bell curve\\n 2. Assuming u is mean and s is the standard deviation of a particular data then its probability density function is (1/(s*sqrt(2*pi))*exp(-0.5*((x-u)/s)^2) \\n 3. Unlike binomial distribution, it can be applied to event which gives any possible outcome (even fractional value) so it is continuos probabilty distribution \\n 4. It will fit for majority of natural events ')\n",
    "# Binomial distribution\n",
    "print('Binomial distribution : \\n 1. It is applied only to those events which has only two mutual exculsive(cant occure same time)outcomes which have same probaility of occurance \\n ex: tossing a coin which outcomes either HEADS or TAILS so it is discrete distribution function \\n 2. Assuming p is probality of sucess, then its probability density function for observing x sucesses in N trails is NCx*p^x*(1-p)^(N-x) , where C represents the combination ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I had written about this on the group as well and shall write it once again that statistics is a very important element of machine learning. While we are working things on Python in this project, you should be working on things in statistics and trying to find statistical conclusions and means in the project. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To center the data (make it have zero mean and unit standard error), you subtract the mean and then divide the result by the standard deviation.\n",
    "\n",
    "x′=(x−μ)/σ.\n",
    "\n",
    "You do that on the training set of data. But then you have to apply the same transformation to your testing set (e.g. in cross-validation), or to newly obtained examples before forecast. But you have to use the same two parameters μ and σ (values) that you used for centering the training set.\n",
    "\n",
    "Hence, every sklearn's transform's fit() just calculates the parameters (e.g. μ and σ in case of StandardScaler) and saves them as an internal objects state. Afterwards, you can call its transform() method to apply the transformation to a particular set of examples.\n",
    "\n",
    "fit_transform() joins these two steps and is used for the initial fitting of parameters on the training set x, but it also returns a transformed x′. Internally, it just calls first fit() and then transform() on the same data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc=StandardScaler()\n",
    "X_train=sc.fit_transform(X_train)\n",
    "X_test=sc.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have applied fit transform on our test and train data. Now it is time to read about various models that can be used to predict whether a cancer cell is beningn or malignant. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model selection is an important part of solving a machine learning but not as important as data cleaning! Your model will always be as good as your data is so focus should always be on getting high quality data and cleaning it properly. \n",
    "\n",
    "There are a number of Machine Learning models avaialble which can be employed to read to meaningful conclusions and selecting the right model depends on a variety of factors such as:\n",
    "\n",
    "1. The accuracy of the model.\n",
    "2. The interpretability of the model.\n",
    "3. The complexity of the model.\n",
    "4. The scalability of the model.\n",
    "5. How long does it take to build, train, and test the model?\n",
    "6. How long does it take to make predictions using the model?\n",
    "7. Does the model meet the business goal?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we will be mainly focussing on three algorithms which can be used to model our dataset:\n",
    "    1. Logistics regression\n",
    "    2. Decision tree classifier \n",
    "    3. Random Forest classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. It is a binary classification method, which will be used when outcome is either 0 or 1\n",
      " 2. If the input data set has features x1 , x2 -- xn decide the binary output y, then y is realted to features as y= sigmoid(Z) where Z = a0 + a1*x1 + a2*x2 -- an*xn (a1,a2 --an are parameter of the model)\n",
      " sigmiod(Z) = 1/(1+exp^(-Z)) = 1 if Z>=0.5 and = 0 if Z< 0.5\n",
      " 3. the parameters(a1,a2--an) can be estimated from trainig data using optimizing technqiues like maximum likelihood estimation (gradeint descent alogorithm is most used), least square estimation\n",
      " 4. R^2 measurment is used to estimates regression performance\n",
      " 5. Advantages: performs well on linear seperable data and it is easy to implement\n",
      " disadvantages: cant perform well when the input data is non linear in nature, It tends to overfit when number of observations are less than number of features\n"
     ]
    }
   ],
   "source": [
    "# Logistic regression\n",
    "print('1. It is a binary classification method, which will be used when outcome is either 0 or 1\\n 2. If the input data set has features x1 , x2 -- xn decide the binary output y, then y is realted to features as y= sigmoid(Z) where Z = a0 + a1*x1 + a2*x2 -- an*xn (a1,a2 --an are parameter of the model)\\n sigmiod(Z) = 1/(1+exp^(-Z)) = 1 if Z>=0.5 and = 0 if Z< 0.5\\n 3. the parameters(a1,a2--an) can be estimated from trainig data using optimizing technqiues like maximum likelihood estimation (gradeint descent alogorithm is most used), least square estimation\\n 4. R^2 measurment is used to estimates regression performance\\n 5. Advantages: performs well on linear seperable data and it is easy to implement\\n disadvantages: cant perform well when the input data is non linear in nature, It tends to overfit when number of observations are less than number of features' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. It is a classification or regression method, in which a series of questions to be posed to label the input raw data ex: if a set of movie names are given then we can pose a question like which are the movies from bollywood ? or which are the movies thriller based? to lable the movies \n",
      " 2. There are three parts of decison tree: root node(raw data), internal decision nodes(derived by posing a series of questions on incoming data) and leaf(final labled data)\n",
      " 3. There are several methods like ID3, C4.5 and CART to make decision tree classification and the core motive of all the methods is to decide which questions to be posed at what stage of tree? \n",
      " 4. In CART(classification and regression Tree)method, Information Gain of each question is calculated at every internal node to select the best question to be asked based on gini impurity or entropy . This process will be continued at each node until we have no further question to be asked. Gini impurity or entopy is a measure of how often a randomly chosen element from the set would be incorrectly labled \n",
      " 5. Advantages : this method doesnt required any initial assumptions and less data processing (ex: additional methods for feature selection is not required) \n",
      " disadvantage: This method tends to overfit and not preffered to predict continous values \n"
     ]
    }
   ],
   "source": [
    "# Decision Tree\n",
    "print('1. It is a classification or regression method, in which a series of questions to be posed to label the input raw data ex: if a set of movie names are given then we can pose a question like which are the movies from bollywood ? or which are the movies thriller based? to lable the movies \\n 2. There are three parts of decison tree: root node(raw data), internal decision nodes(derived by posing a series of questions on incoming data) and leaf(final labled data)\\n 3. There are several methods like ID3, C4.5 and CART to make decision tree classification and the core motive of all the methods is to decide which questions to be posed at what stage of tree? \\n 4. In CART(classification and regression Tree)method, Information Gain of each question is calculated at every internal node to select the best question to be asked based on gini impurity or entropy . This process will be continued at each node until we have no further question to be asked. Gini impurity or entopy is a measure of how often a randomly chosen element from the set would be incorrectly labled \\n 5. Advantages : this method doesnt required any initial assumptions and less data processing (ex: additional methods for feature selection is not required) \\n disadvantage: This method tends to overfit and not preffered to predict continous values ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. This method operated by constructing multiple decision trees on the input data and the decision of majority of trees is choosen as final decision to avoid overfitting of the model due to single decision tree. \n",
      " 2. Different number of datasets are created based on input data using techinque called bootstramp aggregrating or bagging. this number can be decided based on size of the input data set. \n",
      " 3.Each bagging data set has sample from input data set with replacement and expected to have 63.2% input raw data. \n",
      " 4. Bagging in way produces different uncorrelated data sets to avoid overfitting of the model. \n",
      " 5. Advantages: No overfitting, high accuracy. Can maintain accuracy if large portion of data is missing\n",
      " disadvantages: time consuming\n"
     ]
    }
   ],
   "source": [
    "# Random forest\n",
    "print('1. This method operated by constructing multiple decision trees on the input data and the decision of majority of trees is choosen as final decision to avoid overfitting of the model due to single decision tree. \\n 2. Different number of datasets are created based on input data using techinque called bootstramp aggregrating or bagging. this number can be decided based on size of the input data set. \\n 3.Each bagging data set has sample from input data set with replacement and expected to have 63.2% input raw data. \\n 4. Bagging in way produces different uncorrelated data sets to avoid overfitting of the model. \\n 5. Advantages: No overfitting, high accuracy. Can maintain accuracy if large portion of data is missing\\n disadvantages: time consuming')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2: Selectively write 5 lines about each of the above three algorithms so that even a rather inexperienced person can understand it alongwith dealing all the technicalities. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This week's assignment is more theory oriented because communication skills are equally important in Data Science as is coding and Statistics (Mathematics). These questions will help you gain a command over explaining things. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is all for week 3, we will apply these algorithms in the week 4 and calculate accuracy of the models and the final submission and evaluation will be done after that week. All the best!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
